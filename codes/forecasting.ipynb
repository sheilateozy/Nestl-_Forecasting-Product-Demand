{"cells":[{"cell_type":"code","source":["!pip install joblibspark\n!pip install category_encoders"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c8884e40-4ce2-4a8b-8bd7-2803cdc67f35"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Collecting joblibspark\r\n  Downloading joblibspark-0.3.0-py3-none-any.whl (14 kB)\r\nRequirement already satisfied: joblib&gt;=0.14 in /databricks/python3/lib/python3.8/site-packages (from joblibspark) (0.17.0)\r\nInstalling collected packages: joblibspark\r\nSuccessfully installed joblibspark-0.3.0\r\n<span class=\"ansi-yellow-fg\">WARNING: You are using pip version 20.2.4; however, version 21.2.4 is available.\r\nYou should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.</span>\r\nCollecting category_encoders\r\n  Downloading category_encoders-2.2.2-py2.py3-none-any.whl (80 kB)\r\n\r     |████                            | 10 kB 16.6 MB/s eta 0:00:01\r     |████████▏                       | 20 kB 22.7 MB/s eta 0:00:01\r     |████████████▏                   | 30 kB 21.1 MB/s eta 0:00:01\r     |████████████████▎               | 40 kB 12.4 MB/s eta 0:00:01\r     |████████████████████▎           | 51 kB 12.3 MB/s eta 0:00:01\r     |████████████████████████▍       | 61 kB 13.7 MB/s eta 0:00:01\r     |████████████████████████████▍   | 71 kB 12.3 MB/s eta 0:00:01\r     |████████████████████████████████| 80 kB 6.5 MB/s \r\nRequirement already satisfied: pandas&gt;=0.21.1 in /databricks/python3/lib/python3.8/site-packages (from category_encoders) (1.1.3)\r\nRequirement already satisfied: scipy&gt;=1.0.0 in /databricks/python3/lib/python3.8/site-packages (from category_encoders) (1.5.2)\r\nRequirement already satisfied: numpy&gt;=1.14.0 in /databricks/python3/lib/python3.8/site-packages (from category_encoders) (1.19.2)\r\nRequirement already satisfied: scikit-learn&gt;=0.20.0 in /databricks/python3/lib/python3.8/site-packages (from category_encoders) (0.23.2)\r\nRequirement already satisfied: patsy&gt;=0.5.1 in /databricks/python3/lib/python3.8/site-packages (from category_encoders) (0.5.1)\r\nRequirement already satisfied: statsmodels&gt;=0.9.0 in /databricks/python3/lib/python3.8/site-packages (from category_encoders) (0.12.0)\r\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in /databricks/python3/lib/python3.8/site-packages (from pandas&gt;=0.21.1-&gt;category_encoders) (2.8.1)\r\nRequirement already satisfied: pytz&gt;=2017.2 in /databricks/python3/lib/python3.8/site-packages (from pandas&gt;=0.21.1-&gt;category_encoders) (2020.5)\r\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn&gt;=0.20.0-&gt;category_encoders) (2.1.0)\r\nRequirement already satisfied: joblib&gt;=0.11 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn&gt;=0.20.0-&gt;category_encoders) (0.17.0)\r\nRequirement already satisfied: six in /usr/local/lib/python3.8/site-packages (from patsy&gt;=0.5.1-&gt;category_encoders) (1.15.0)\r\nInstalling collected packages: category-encoders\r\nSuccessfully installed category-encoders-2.2.2\r\n<span class=\"ansi-yellow-fg\">WARNING: You are using pip version 20.2.4; however, version 21.2.4 is available.\r\nYou should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.</span>\r\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Collecting joblibspark\r\n  Downloading joblibspark-0.3.0-py3-none-any.whl (14 kB)\r\nRequirement already satisfied: joblib&gt;=0.14 in /databricks/python3/lib/python3.8/site-packages (from joblibspark) (0.17.0)\r\nInstalling collected packages: joblibspark\r\nSuccessfully installed joblibspark-0.3.0\r\n<span class=\"ansi-yellow-fg\">WARNING: You are using pip version 20.2.4; however, version 21.2.4 is available.\r\nYou should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.</span>\r\nCollecting category_encoders\r\n  Downloading category_encoders-2.2.2-py2.py3-none-any.whl (80 kB)\r\n████                            | 10 kB 16.6 MB/s eta 0:00:01\r     |████████▏                       | 20 kB 22.7 MB/s eta 0:00:01\r     |████████████▏                   | 30 kB 21.1 MB/s eta 0:00:01\r     |████████████████▎               | 40 kB 12.4 MB/s eta 0:00:01\r     |████████████████████▎           | 51 kB 12.3 MB/s eta 0:00:01\r     |████████████████████████▍       | 61 kB 13.7 MB/s eta 0:00:01\r     |████████████████████████████▍   | 71 kB 12.3 MB/s eta 0:00:01\r     |████████████████████████████████| 80 kB 6.5 MB/s \r\nRequirement already satisfied: pandas&gt;=0.21.1 in /databricks/python3/lib/python3.8/site-packages (from category_encoders) (1.1.3)\r\nRequirement already satisfied: scipy&gt;=1.0.0 in /databricks/python3/lib/python3.8/site-packages (from category_encoders) (1.5.2)\r\nRequirement already satisfied: numpy&gt;=1.14.0 in /databricks/python3/lib/python3.8/site-packages (from category_encoders) (1.19.2)\r\nRequirement already satisfied: scikit-learn&gt;=0.20.0 in /databricks/python3/lib/python3.8/site-packages (from category_encoders) (0.23.2)\r\nRequirement already satisfied: patsy&gt;=0.5.1 in /databricks/python3/lib/python3.8/site-packages (from category_encoders) (0.5.1)\r\nRequirement already satisfied: statsmodels&gt;=0.9.0 in /databricks/python3/lib/python3.8/site-packages (from category_encoders) (0.12.0)\r\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in /databricks/python3/lib/python3.8/site-packages (from pandas&gt;=0.21.1-&gt;category_encoders) (2.8.1)\r\nRequirement already satisfied: pytz&gt;=2017.2 in /databricks/python3/lib/python3.8/site-packages (from pandas&gt;=0.21.1-&gt;category_encoders) (2020.5)\r\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn&gt;=0.20.0-&gt;category_encoders) (2.1.0)\r\nRequirement already satisfied: joblib&gt;=0.11 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn&gt;=0.20.0-&gt;category_encoders) (0.17.0)\r\nRequirement already satisfied: six in /usr/local/lib/python3.8/site-packages (from patsy&gt;=0.5.1-&gt;category_encoders) (1.15.0)\r\nInstalling collected packages: category-encoders\r\nSuccessfully installed category-encoders-2.2.2\r\n<span class=\"ansi-yellow-fg\">WARNING: You are using pip version 20.2.4; however, version 21.2.4 is available.\r\nYou should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.</span>\r\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["import pandas as pd\nimport numpy as np\nfrom category_encoders.hashing import HashingEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.utils import parallel_backend  \nfrom sklearn.inspection import permutation_importance\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom joblib import Parallel, delayed, parallel_backend, Memory\nfrom joblibspark import register_spark\nfrom sklearn.utils import parallel_backend  \nfrom pyspark.sql.types import *\nimport datetime"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"105fbbab-e33e-48ed-9171-716f5d65a720"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_mat_level_data(df):  #takes in a spark dataframe\n  #convert all col names to lowercase\n  for col in df.columns:\n      df = df.withColumnRenamed(col, col.lower())\n\n  #convert to pandas for easier manip\n  df = df.toPandas()\n\n  #convert 'week' col into datetime object so can subset by date easier later\n  df['week'] = pd.to_datetime(df['week'])\n\n  #retain only cols required for modelling\n  cols_to_drop = ['pl2_business_id', 'min_date', 'month', 'cust_lev6', 'trend']\n  df.drop(cols_to_drop, axis='columns', inplace=True)\n  \n  #extract year \n  df['year'] = df['week'].apply(lambda x: str(x.year))\n  \n  #one-hot encode year col\n  year_dummies_df = pd.get_dummies(df['year'])\n  df = pd.concat([df, year_dummies_df], axis=1)\n  df.drop(['year'], axis='columns', inplace=True)\n\n  #extract week number of year\n  df['week_num_in_year'] = df['week'].apply(lambda x: x.week)\n\n  #hash encode week number since it is a high-cardinality (50+ levels) categorical variable\n  he = HashingEncoder(n_components=30, cols=['week_num_in_year'])  #reduce dimensionality of newly created cols from 50+ to 30. see microsoft word doc documentation for motivation\n  df = he.fit_transform(df)\n  \n  #groupby material first\n  df_material = list(df.groupby('material'))\n\n  #instantiate empty dfs\n  all_promo = pd.DataFrame(columns=df.columns)\n  all_nopromo = pd.DataFrame(columns=[col for col in df.columns if 'sales' not in col])\n  \n  for i in df_material:\n    mat_df = i[1]\n  \n    #output 2 things: timeseries with promo and timeseries without promo\n    with_promo = mat_df[mat_df['promo'] == 'YES'].reset_index(drop=True)\n    without_promo = mat_df[mat_df['promo'] == 'NO'].reset_index(drop=True)\n    with_promo.drop(['promo'], axis='columns', inplace=True)\n    without_promo.drop(['promo'], axis='columns', inplace=True)\n\n    #drop rows in with_promo that have all sales cols as null, move them to without_promo instead\n    sales_cols = [col for col in df.columns if 'sales' in col]\n    sales_cols_nulls = with_promo[sales_cols].isnull().apply(lambda row: row.sum(), axis=1)  \n    rows_to_keep = sales_cols_nulls[sales_cols_nulls < len(sales_cols)].index\n    rows_to_throw = sales_cols_nulls[sales_cols_nulls == len(sales_cols)].index\n\n    without_promo = pd.concat([without_promo, with_promo.reindex(index=rows_to_throw)])\n    with_promo = with_promo.reindex(index=rows_to_keep)\n\n    #drop sales cols in without_promo since all are null\n    without_promo.drop(sales_cols, axis='columns', inplace=True)\n  \n    #add to all_promo and all_no_promo\n    all_promo = pd.concat([all_promo, with_promo])\n    all_nopromo = pd.concat([all_nopromo, without_promo])\n  \n  #do groupby and aggregate\n  groupby_cols = ['material', 'col_0', 'col_1', 'col_2', 'col_3', 'col_4', 'col_5', 'col_6', 'col_7',\n       'col_8', 'col_9', 'col_10', 'col_11', 'col_12', 'col_13', 'col_14',\n       'col_15', 'col_16', 'col_17', 'col_18', 'col_19', 'col_20', 'col_21',\n       'col_22', 'col_23', 'col_24', 'col_25', 'col_26', 'col_27', 'col_28',\n       'col_29', 'week', 'valentines', 'valentines_lag1', 'thanksgiving', 'thanksgiving_lag1',\n       'thanksgiving_lag2', 'halloween', 'halloween_lag1', 'easter',\n       'easter_lag1', 'easter_lag2', 'easter_lag3', 'christmas',\n       'christmas_lag1', 'christmas_lag2', 'christmas_lag3',\n       'independence_day', 'independence_day_lag1', 'independence_day_lag2',\n       'independence_day_lag3', 'playoffs', 'playoffs_lag1', 'superbowl',\n       'superbowl_lag1', 'newyear', 'm1', 'm2', 'm3', 'm4', 'm5', 'm6', 'm7',\n       'm8', 'm9', 'm10', 'm11', 'm12', '2016', '2017', '2018', '2019',\n       '2020', '2021', '2022', '2023']\n\n  with_promo_agg = all_promo.groupby(groupby_cols).agg({col: 'sum' for col in all_promo.columns if ('order' in col) | ('sales' in col)}).reset_index()\n\n  without_promo_agg = all_nopromo.groupby(groupby_cols).agg({col: 'sum' for col in all_nopromo.columns if 'order' in col}).reset_index()\n  \n  return list(with_promo_agg.groupby('material')), list(without_promo_agg.groupby('material'))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f693ea6d-0e28-429d-88eb-463c5a4b63b6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_all_predictions(biz_name, whether_promo, mat_timeseries_list):  \n  location = f'{biz_name}_cachedir'\n  memory = Memory(location=location, verbose=0)\n\n  @memory.cache(verbose=0)  #store output of this function in memory for each i\n  def for_i_in_all_timeseries_list(i):  \n    material = i[0]\n    timeseries_df = i[1]\n\n    try:\n      predictions_per_time_series = get_predictions_per_time_series(material, timeseries_df)\n      \n      if len(predictions_per_time_series) > 0:  #could have 0 rows then will get error when saving file to DBFS\n        dbutils.fs.rm(f'dbfs:/FileStore/phase 2/all mat level/{biz_name}/{material}_{whether_promo}.csv', True)  #remove any file with same name alr there just in case, otherwise will get error when saving the file to DBFS\n        spark.createDataFrame(predictions_per_time_series).coalesce(1).write.format('com.databricks.spark.csv').option('header', 'true').save(f'dbfs:/FileStore/phase 2/all mat level/{biz_name}/{material}_{whether_promo}.csv') \n        \n      else:\n        print(f'note {material}, {whether_promo}: no content to save!')\n\n    except Exception as e:\n      print(f'error: material={material}')\n      print(e)\n\n  #run the above helper fn in parallel\n  with parallel_backend(backend='threading', n_jobs=-1):  \n    Parallel(verbose=1000)(delayed(for_i_in_all_timeseries_list)(i) for i in mat_timeseries_list)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8eb78e09-99f8-44cd-8928-a5cf4d94f9e1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_predictions_per_time_series(material, timeseries_df):\n  #create df skeleton to store predictions later\n  all_predictions_df = pd.DataFrame(columns=['material', 'date', 'forecast week', 'lag weeks', 'actual', 'prediction'])\n  \n  #there are 11 forecast periods\n  all_forecast_weeks = ['26 May 2019', '23 Jun 2019', '28 Jul 2019', '25 Aug 2019', '29 Sep 2019', '27 Oct 2019', '24 Nov 2019', '30 Dec 2019', '29 Dec 2019', '26 Jan 2020', '23 Feb 2020']\n  \n  #tune rf and select features to be used for all 11 forecasting periods for this timeseries. use the largest amnt of training data to do these\n  overall_train = timeseries_df[timeseries_df['week'] < '13 Feb 2022']  #103 weeks after last forecast period of 23 Feb 2020\n  overall_test = timeseries_df[timeseries_df['week'] >= '13 Feb 2022']  #overall_test not used, only instantiated bc clean_up_train_test() fn below needs it as argument too\n  \n  #clean up overall_train and overall_test for fitting rf\n  overall_train, overall_test = clean_up_train_test(overall_train, overall_test)\n    \n  #tune rf\n  initial_rf, opt_min_samples_leaf, opt_min_samples_split, opt_max_depth, initial_val_dpa = tune_rf(overall_train)\n\n  #select features based on tuned rf\n  features_selected_list, opt_rf = select_features(initial_rf, opt_min_samples_leaf, opt_min_samples_split, opt_max_depth, initial_val_dpa, overall_train) \n  \n  #get 11 forecasts for this timeseries\n  for week in all_forecast_weeks:  \n    #subset data into training and test\n    train = timeseries_df[timeseries_df['week'] < week]\n    test = timeseries_df[(timeseries_df['week'] >= week) & (timeseries_df['week'] <= datetime.datetime.strptime(week, '%d %b %Y') + datetime.timedelta(weeks=103))]  #test set to obtain predictions for is 103 weeks ahead\n    \n    if (len(train) > 0) & (len(test) > 0): \n      #organize this forecast period's predictions to be added on to all_predictions_df later\n      per_period_predictions_df = pd.DataFrame(columns=['material', 'date', 'forecast week', 'lag weeks', 'actual', 'prediction'])\n      per_period_predictions_df['date'] = test['week']\n      per_period_predictions_df['lag weeks'] = round(per_period_predictions_df['date'].apply(lambda date: (date-datetime.datetime.strptime(week, '%d %b %Y')).days / 7)) \n      per_period_predictions_df['actual'] = test['order_quantity']\n      per_period_predictions_df.dropna(subset=['actual'], inplace=True)  #drop rows with null in actual cause we won't be predicting for them. note that having null in actual does not refer to 0 value, it refers to the actual NAs\n      per_period_predictions_df['material'] = material\n      per_period_predictions_df['forecast week'] = week\n\n      #clean up train and test for fitting rf\n      train, test = clean_up_train_test(train, test)\n\n      #get predictions for test set\n      predictions = get_predictions_per_forecast_period(train, test, opt_rf, features_selected_list)\n\n      #add predictions to per_period_predictions_df\n      per_period_predictions_df['prediction'] = predictions\n\n      #add per_period_predictions_df to all_predictions_df\n      all_predictions_df = pd.concat([all_predictions_df, per_period_predictions_df], axis=0)\n      \n    else:\n      pass  \n    \n  return all_predictions_df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83f54ac6-999d-4d17-90c3-600ed65b666a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def clean_up_train_test(train, test):\n  #remove all rows with no actual order quantity\n  test.dropna(subset=['order_quantity'], inplace=True)  #we won't waste time predicting for rows without an actual to compare to\n  train.dropna(subset=['order_quantity'], inplace=True)  #we can't learn from rows without target\n    \n  #drop cols with >40% of rows being nulls bc the col is useless\n  for col in train.columns:\n    if train[col].isnull().sum() >= 0.4*len(train):\n      train.drop([col], axis='columns', inplace=True)\n      test.drop([col], axis='columns', inplace=True)  #must drop for test also!\n\n  if train.isnull().sum().sum() != 0:  #if there still exists nulls in train after doing the above\n    train.fillna(method='ffill', axis='index', inplace=True)  #use forward fill imputation \n    if train.isnull().sum().sum() != 0:  #if there still exists nulls, which is the case if the very first value starts with null so forward fill imputation does not work\n      train.fillna(method='bfill', axis='index', inplace=True)  #do backward fill imputation \n      if train.isnull().sum().sum() != 0:  ##if there still exists nulls, which is the case if both the very first value and the very last value start with null so both forward fill and backward fill imputation do not work\n          train = train.apply(lambda col: col.fillna(col.mean()), axis=0)  #do mean imputation \n          \n  #repeat the above for nulls in test set\n  if test.isnull().sum().sum() != 0:  \n    test.fillna(method='ffill', axis='index', inplace=True)\n    if test.isnull().sum().sum() != 0:  \n      test.fillna(method='bfill', axis='index', inplace=True)  \n      if test.isnull().sum().sum() != 0:  \n          test = test.apply(lambda col: col.fillna(col.mean()), axis=0)  \n  \n  #subset to only features needed for modelling\n  train.drop(['material', 'week'], axis='columns', inplace=True)\n  test.drop(['material', 'week'], axis='columns', inplace=True)\n  \n  return train, test"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"22b431f9-ba8d-4e0c-a40d-7b4e9b1607c2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def tune_rf(train): \n  feature_cols = [col for col in train.columns if col != 'order_quantity']\n  X_train = train[feature_cols]\n  y_train = train['order_quantity']\n\n  rf = RandomForestRegressor(n_estimators=300, random_state=15)  \n  param_grid = {'min_samples_leaf': [2, 5, 8], 'min_samples_split': [5, 10, 15], 'max_depth': [5, 8, 11, 14, 17], 'criterion': ['mae', 'mse']}  #tune the most important hyperparameters in a random forest model  \n  \n  #make custom scoring metric: DPA\n  def dpa(y_true, y_pred):\n    abs_diff = np.abs(y_pred - y_true)\n    if y_true.sum() != 0:\n      return 1 - abs_diff.sum()/y_true.sum()\n    else:\n      return 1 - abs_diff.sum()\n\n  dpa_scorer = make_scorer(dpa, greater_is_better=True)\n\n  timeseries_cv = TimeSeriesSplit(n_splits=5)  \n  rs_rf = RandomizedSearchCV(rf, param_grid, cv=timeseries_cv, scoring=dpa_scorer, refit=True, return_train_score=True, random_state=15, n_iter=150)\n\n  register_spark()\n  with parallel_backend('spark', n_jobs=20):  #can't put n_jobs too high bc alr got outer parallelization going on in get_all_predictions() function, will hit max spark worker capacity and obtain error\n    rs_rf.fit(X_train, y_train)  \n\n  #get best hyperparams\n  best_params = rs_rf.best_params_   \n  opt_min_samples_leaf = best_params['min_samples_leaf']\n  opt_min_samples_split = best_params['min_samples_split']\n  opt_max_depth = best_params['max_depth']\n  opt_criterion = best_params['criterion']\n  \n  #get mean validation DPA \n  val_dpa = rs_rf.best_score_\n\n  return rs_rf, opt_min_samples_leaf, opt_min_samples_split, opt_max_depth, opt_criterion, val_dpa  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1b84f49-42d4-4909-89e9-818b53ef2773"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def select_features(rs_rf, opt_min_samples_leaf, opt_min_samples_split, opt_max_depth, opt_criterion, initial_val_dpa, train):\n  feature_cols = [col for col in train.columns if col != 'order_quantity']\n  X_train = train[feature_cols]\n  y_train = train['order_quantity']\n  \n  pi = permutation_importance(rs_rf, X_train, y_train, n_repeats=5, random_state=15)  \n  all_imptances = list(pi.importances_mean)  #this result is not ordered, so we order it below\n  feature_impt_dic = {}\n  for num in range(len(all_imptances)):\n    feature = X_train.columns[num]\n    impt = all_imptances[num]\n    feature_impt_dic[feature] = impt\n  feature_impt_dic_sorted = dict(sorted(feature_impt_dic.items(), key=lambda tup: tup[1], reverse=True))\n\n#test feature subsets of: (all features, 85, 75, 65, 55, 35)\n  num_features_to_test = [85, 75, 65, 55, 35]\n  \n  #create df skeleton to store feature selection results\n  feature_selection_results = pd.DataFrame(columns=['features selected', 'validation DPA'])\n\n  for num_features in num_features_to_test:\n    try:  #need try-except bc nopromo df might not have 85 features since it does not have any sales columns as features\n      selected_features = list(feature_impt_dic_sorted.keys())[:num_features]\n      num_total_features = len(X_train.columns)\n\n      #tune rf on selected features\n      selected_features.append('order_quantity')  #need 'selected_features' to have target to pass new train & test into tune_rf()\n      val_dpa = train_rf_per_feature_subset(train[selected_features], opt_min_samples_leaf, opt_min_samples_split, opt_max_depth, opt_criterion)\n\n      #add results to results df\n      selected_features.pop()  #remove target from selected_features list that was just added from above\n      feature_selection_results.loc[num_features] = [selected_features, val_dpa]  \n      \n    except Exception as e:  \n      print(e)\n      continue\n      \n  #last comparison: using no threshold = no feature selection at all\n  feature_selection_results.loc[num_total_features] = [list(X_train.columns), initial_val_dpa]\n    \n  #decide which number of features is the best to use\n  max_validation_DPA = feature_selection_results['validation DPA'].max() \n  \n  #get rf and features selected associated with best threshold\n  best_num_features = feature_selection_results.index[feature_selection_results['validation DPA'] == max_validation_DPA].tolist()[0]\n  opt_rf = all_rfs[best_num_features]\n  features_selected_list = feature_selection_results.loc[best_num_features]['features selected']\n  \n  return features_selected_list"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"43a0c274-a3ea-4faa-a092-bd4985d87c34"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def train_rf_per_feature_subset(train, opt_min_samples_leaf, opt_min_samples_split, opt_max_depth, opt_criterion):\n  feature_cols = [col for col in train.columns if col != 'order_quantity']\n  X_train = train[feature_cols]\n  y_train = train['order_quantity']\n\n  rf = RandomForestRegressor(n_estimators=300, random_state=15, criterion='mae')  \n  param_grid = {'min_samples_leaf': [opt_min_samples_leaf], 'min_samples_split': [opt_min_samples_split], 'max_depth': [opt_max_depth], 'criterion': [opt_criterion]} \n  timeseries_cv = TimeSeriesSplit(n_splits=5) \n\n  #make custom scoring metric: DPA\n  def dpa(y_true, y_pred):\n    abs_diff = np.abs(y_pred - y_true)\n    if y_true.sum() != 0:\n      return 1 - abs_diff.sum()/y_true.sum()\n    else:\n      return 1 - abs_diff.sum()\n  dpa_scorer = make_scorer(dpa, greater_is_better=True)\n  \n  rs_rf = RandomizedSearchCV(rf, param_grid, cv=timeseries_cv, scoring=dpa_scorer, refit=True, n_iter=1)  #use randomized search with only n_iter=1 because we have only 1 hyperparameter combination passed into the parameter grid for tuning. however, we want to use randomized search still because it has a built-in cross-validation function, so we can obtain the mean validation DPA for the feature subset we are testing. this mean validation DPA is more likely to be closer to the true validation DPA of using this feature subset on new unseen data hence we want to obtain it\n  \n  register_spark()\n  with parallel_backend('spark', n_jobs=20):  #can't put n_jobs too high bc alr got outer parallelization going on in get_all_predictions() function, will hit max spark worker capacity\n    rs_rf.fit(X_train, y_train)  \n    \n  #get mean validation DPA\n  val_dpa = rs_rf.best_score_\n  \n  return val_dpa"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d9494d1a-85c7-4ded-91e9-90b36c05e037"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_predictions_per_forecast_period(train, test, opt_min_samples_leaf, opt_min_samples_split, opt_max_depth, opt_criterion, features_selected_list):\n  feature_cols = [col for col in train.columns if col != 'order_quantity']\n  X_train = train[feature_cols]\n  y_train = train['order_quantity']\n  X_test = test[feature_cols]  \n  \n  #fit rf on training set\n  rf = RandomForestRegressor(n_estimators=300, random_state=15, min_samples_leaf=opt_min_samples_leaf, min_samples_split=opt_min_samples_split, max_depth=opt_max_depth, criterion=opt_criterion)\n  \n  register_spark()\n  with parallel_backend('spark', n_jobs=20):  #can't put n_jobs too high bc alr got outer parallelization going on in get_all_predictions() function, will hit max spark worker capacity\n    rf.fit(X_train, y_train)  \n    predictions = rf.predict(X_test[features_selected_list])\n  \n  return predictions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"73b3b7e3-a0a9-445a-9373-41171b01e436"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#check that all material have a file and the file has things in it\n#run once for promo_mat_timeseries_list and once for nopromo_mat_timeseries_list\n\ndef check_get_all_predictions(biz_name, whether_promo, mat_timeseries_list):\n  schema = StructType([StructField('material', IntegerType(), True), StructField('date', TimestampType(), True), StructField('forecast week', StringType(), True), StructField('lag weeks', DoubleType(), True), StructField('actual', DoubleType(), True), StructField('prediction', DoubleType(), True)])\n  \n  mat_errors = []\n  for i in mat_timeseries_list:\n    material = i[0]\n\n    try: \n      df = spark.read.format(\"csv\") \\\n        .schema(schema) \\\n        .option(\"header\", \"true\") \\\n        .option(\"sep\", \",\") \\\n        .load(f\"/FileStore/phase 2/all mat level/{biz_name}/{material}_{whether_promo}.csv\").toPandas()   #tests that file exists\n      \n      df.iloc[0]  #tests that file contains required results if it exists\n\n    except Exception as e:\n      print(material)\n      print(e)\n      mat_errors.append(material)\n    \n  return mat_errors"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e4517296-78f7-4c8c-b014-542a7f53eb5a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#re-run materials that had errors above\n#run once for promo_mat_timeseries_list and once for nopromo_mat_timeseries_list\n\ndef run_errors(biz_name, whether_promo, all_mat_errors, mat_timeseries_list):\n  for material in all_mat_errors:\n    for i in mat_timeseries_list:\n      if i[0] == material:\n        mat_df = i[1]\n        break\n\n    try:\n        predictions_per_time_series = get_predictions_per_time_series(material, mat_df)\n\n        if len(predictions_per_time_series) > 0:\n          dbutils.fs.rm(f'dbfs:/FileStore/phase 2/all mat level/{biz_name}/{material}_{whether_promo}.csv', True)  #remove any file with same name alr there just in case, otherwise will get error when saving the file to DBFS\n          spark.createDataFrame(predictions_per_time_series).coalesce(1).write.format('com.databricks.spark.csv').option('header', 'true').save(f'dbfs:/FileStore/phase 2/all mat level/{biz_name}/{material}_{whether_promo}.csv') \n\n        else:\n          print(f'note {material}, {whether_promo}: no content to save!')\n          continue\n\n    except Exception as e:\n      print(material)\n      print(e)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2f5650b1-7c23-4865-be2d-31ee3be5b087"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_required_result(biz_name, promo_mat_timeseries_list, nopromo_mat_timeseries_list): \n  schema = StructType([StructField('material', IntegerType(), True), StructField('date', TimestampType(), True), StructField('forecast week', StringType(), True), StructField('lag weeks', DoubleType(), True), StructField('actual', DoubleType(), True), StructField('prediction', DoubleType(), True)])\n\n  all_predictions = pd.DataFrame(columns=['material', 'date', 'forecast week', 'lag weeks', 'actual', 'prediction'])\n  \n  for timeseries in promo_mat_timeseries_list:\n    material = timeseries[0]\n    \n    try:\n      df = spark.read.format(\"csv\") \\\n            .schema(schema) \\\n            .option(\"header\", \"true\") \\\n            .option(\"sep\", \",\") \\\n            .load(f\"/FileStore/phase 2/all mat level/{biz_name}/{material}_promo.csv\").toPandas()\n\n      all_predictions = pd.concat([all_predictions, df])\n    except Exception as e:  #some exceptions are okay bc some materials have no content to be saved!\n      print(material)\n      print(e)\n      \n  for timeseries in nopromo_mat_timeseries_list:\n    material = timeseries[0]\n    \n    try:\n      df = spark.read.format(\"csv\") \\\n            .schema(schema) \\\n            .option(\"header\", \"true\") \\\n            .option(\"sep\", \",\") \\\n            .load(f\"/FileStore/phase 2/all mat level/{biz_name}/{material}_nopromo.csv\").toPandas()\n\n      all_predictions = pd.concat([all_predictions, df])\n    except Exception as e:  #some exceptions are okay bc some materials have no content to be saved!\n      print(material)\n      print(e)\n    \n  result = all_predictions.groupby(['material', 'date', 'forecast week', 'lag weeks'])[['actual', 'prediction']].apply(sum).reset_index(inplace=False)\n      \n  #save result to dbfs\n  dbutils.fs.rm(f'dbfs:/FileStore/phase 2/all mat level/{biz_name}/predictions_final.csv', True)  #remove any file with same name alr there just in case, otherwise will get error when saving the file to DBFS\n  spark.createDataFrame(result).coalesce(1).write.format('com.databricks.spark.csv').option('header', 'true').save(f'dbfs:/FileStore/phase 2/all mat level/{biz_name}/predictions_final.csv')\n  \n  return spark.createDataFrame(result)  #to save when displayed"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a222d6e-62cf-4c0d-a536-e83153e686c4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#example of the flow of using the above functions, on USR business\n\nbiz_name = 'usr'\n\npromo_mat_timeseries_list, nopromo_mat_timeseries_list =  get_mat_level_data(usr)  #note: usr is a spark dataframe of the file sales_history_sell_out_usr.csv   \n\nget_all_predictions(biz_name, 'promo', promo_mat_timeseries_list)  \npromo_mat_errors = check_get_all_predictions(biz_name, 'promo', promo_mat_timeseries_list)\nrun_errors(biz_name, 'promo', promo_mat_errors, promo_mat_timeseries_list)\n\nget_all_predictions(biz_name, 'nopromo', nopromo_mat_timeseries_list)  \nnopromo_mat_errors = check_get_all_predictions(biz_name, 'nopromo', nopromo_mat_timeseries_list)\nrun_errors(biz_name, 'nopromo', nopromo_mat_errors, nopromo_mat_timeseries_list)\n\nresult = get_required_result(biz_name, promo_mat_timeseries_list, nopromo_mat_timeseries_list)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2da65455-645c-4c20-ba68-288174c7226b"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Phase 2 Codes (Both Feature Selection & Forecasting at Material Level)","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1056385819762310}},"nbformat":4,"nbformat_minor":0}
