{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "01ed9407-fc19-4300-b01d-f5a0c68d718c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install joblibspark\n",
    "!pip install rfpimp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f9e7b2db-b91c-400b-af45-ff2493292e20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from joblibspark import register_spark\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.utils import parallel_backend  \n",
    "from rfpimp import importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a3d7d4b8-18bb-45ec-8071-4390d2f238c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1) function to obtain cleaned dfp and dfnp from the original dataset, which are rows that do and do not contain sales data (various sales columns in the dataset) respectively. we require to obtain these 2 separate dataframes as we will fit a different random forest model for feature selection to each of these 2 dataframes separately.\n",
    "\n",
    "def obtain_cleaned_dfp_dfnp(df):  #takes in a spark dataframe named df\n",
    "  #convert all col names to lowercase\n",
    "  for col in df.columns:\n",
    "      df = df.withColumnRenamed(col, col.lower())\n",
    "  \n",
    "  #convert any nulls in various order quantity columns to 0. this is applicable to this specific use-case only and can be removed for other datasets\n",
    "  order_quant_cols = [col for col in df.columns if 'order' in col]\n",
    "  df = df.na.fill(value=0, subset=order_quant_cols)\n",
    "\n",
    "  #promo column is a boolean depicting if sales data is available\n",
    "  #separate original df into 2 dfs, promo=YES and promo=NO, named dfp and dfnp respectively\n",
    "  dfp = df.filter(col('promo')=='YES')\n",
    "  dfnp = df.filter(col('promo')=='NO')\n",
    "\n",
    "  #retain only cols required for modelling for dfp and dfnp respectively\n",
    "  cols_to_drop = ['pl2_business_id', 'material', 'cust_lev6', 'week', 'min_date', 'promo', 'month', 'year']\n",
    "  desired_cols = [col for col in df.columns if col not in cols_to_drop]\n",
    "  dfp = dfp.select(desired_cols)\n",
    "\n",
    "  cols_to_drop = ['pl2_business_id', 'material', 'cust_lev6', 'week', 'min_date', 'promo', 'month', 'year'] + [col for col in df.columns if 'sales' in col]\n",
    "  desired_cols = [col for col in df.columns if col not in cols_to_drop]\n",
    "  dfnp = dfnp.select(desired_cols)\n",
    "  \n",
    "  #drop any nulls in any feature. this can be changed to impute nulls instead in other use-cases. however, in our specific use-case here, only few rows contain nulls in features so we can afford to drop them without losing too much data\n",
    "  dfp = dfp.na.drop()\n",
    "  dfnp = dfnp.na.drop()\n",
    "\n",
    "  return (dfp, dfnp)  #note: both dfp and dfnp are spark dataframes, not pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a1e663bb-85b2-463a-ab51-654268a7e5e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2) function to obtain tuned random forest\n",
    "\n",
    "def run_rf(df):  #takes in a spark dataframe\n",
    "  register_spark()  #register spark backend for parallelization later\n",
    "\n",
    "  feature_cols = [col for col in df.columns if col != 'order_quantity']\n",
    "  X = df.select(feature_cols).toPandas()\n",
    "  y = df.select('order_quantity').toPandas()\n",
    "  \n",
    "  #obtain training and test sets. note that since we are working with timeseries data, we cannot simply use scikitlearn's train_test_split() function to do this since rows in the test set must always belong to a time period after rows in the training set to avoid data leakage\n",
    "  train_test_split_index = 0.7*len(df)\n",
    "  train = df.iloc[:train_test_split_index]\n",
    "  test = df.iloc[train_test_split_index:]\n",
    "  X_train = train[feature_cols]\n",
    "  X_test = test[feature_cols]\n",
    "  y_train = train['order_quantity']\n",
    "  y_test = test['order_quantity']\n",
    "\n",
    "  rf = RandomForestRegressor(n_estimators=300, random_state=15, max_features='sqrt', max_samples=0.9)  \n",
    "  param_grid = {'max_depth': [20, 25, 30, 40, 50], 'min_samples_leaf': [1, 2, 4, 8], 'criterion': ['mae', 'mse'], 'min_samples_split': [1, 2, 4, 8]}  #tune the most important hyperparameters of a random forest model\n",
    "  timeseries_cv = TimeSeriesSplit(n_splits=5)  #we cannot use a normal cross-validator since we must ensure rows in the test set must always belong to a time period after rows in the training set to avoid data leakage, hence we use a special cross-validator type here to ensure this\n",
    "  rs_rf = RandomizedSearchCV(rf, param_grid, cv=timeseries_cv, n_iter=120, verbose=10, random_state=15, refit=True)  #n_iter refers to the number of hyperparameter set combinations to test out under this randomized search\n",
    "  \n",
    "  print('starting to tune rf')\n",
    "  \n",
    "  with parallel_backend('spark', n_jobs=15):  #using n_jobs=-1 takes v long to parallelize! so use 15\n",
    "    rs_rf.fit(X_train, y_train)  \n",
    "  \n",
    "  #get best hyperparams\n",
    "  best_params = rs_rf.best_params_   \n",
    "  print(f'best hyperparams are: {best_params}')\n",
    "  \n",
    "  opt_max_depth = best_params['max_depth']\n",
    "  opt_min_samples_leaf = best_params['min_samples_leaf']\n",
    "  opt_min_samples_split = best_params['min_samples_split']\n",
    "  opt_criterion = best_params['criterion']\n",
    "\n",
    "  return ((opt_max_depth, opt_min_samples_leaf, opt_min_samples_split, opt_criterion), (X_train, X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "efa20e71-e2dc-4363-a07e-138445382e1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3) function to obtain permutation importances\n",
    "\n",
    "def get_perm_imptance(df, opt_max_depth, opt_min_samples_leaf, opt_min_samples_split, opt_criterion, X_train, X_test, y_train, y_test, biz_name):\n",
    "  rf = RandomForestRegressor(n_estimators=300, random_state=15, max_features='sqrt', max_samples=0.9, max_depth=opt_max_depth, min_samples_leaf=opt_min_samples_leaf, min_samples_split=opt_min_samples_split, criterion=opt_criterion)  \n",
    "  rf.fit(X_train, y_train)  #takes ~3min\n",
    "  \n",
    "  print('rf built. running perm impt')\n",
    "  \n",
    "  #run perm imptance\n",
    "  #1) working with dfp\n",
    "  feature_cols = [col for col in df.columns if col != 'order_quantity']\n",
    "  if 'sales' in feature_cols:  \n",
    "    all_sales_cols = [col for col in df.columns if 'sales' in col]\n",
    "    all_other_cols = [col for col in df.columns if ('sales' not in col) and (col!='order_quantity')]\n",
    "    \n",
    "    grouped_features = []\n",
    "    grouped_features.append(all_sales_cols)\n",
    "    grouped_features.extend(all_other_cols)\n",
    "\n",
    "    feature_impt = importances(model=rf, X_valid=X_test, y_valid=y_test, features=grouped_features)  #takes 25s. gives a pandas df with rows alr sorted by importance!! (most impt first)\n",
    "    feature_impt.reset_index(inplace=True)\n",
    "    \n",
    "    print('feature impportance obtained. saving it as a file')\n",
    "    \n",
    "    #the importances() function above concatenates all the sales columns' names together since they are passed in as a grouped feature due to high collinearity between the sales columns. change this long concatenated name into simply 'all_sales_cols' for easier reading\n",
    "    row_count = 0\n",
    "    for feature in feature_impt['Feature']:\n",
    "      if 'sales' not in feature:\n",
    "        row_count+=1\n",
    "      else:\n",
    "        break\n",
    "\n",
    "    feature_impt.iloc[row_count, 0] = 'all_sales_cols'\n",
    "  \n",
    "    #save feature_impt to dbfs so we don't lose this after obtaining the results\n",
    "    spark.createDataFrame(feature_impt).coalesce(1).write.format('com.databricks.spark.csv').option('header', 'true').save(f'dbfs:/FileStore/feature_selection_project/{biz_name}_promo.csv')\n",
    "    \n",
    "  \n",
    "  #2) working with dfnp\n",
    "  #dfnp does not contain any sales column since it contains rows that have promo=NO, hence we do not run into the problem of the concatenation of column names above\n",
    "  else:  \n",
    "    \n",
    "    feature_impt = importances(model=rf, X_valid=X_test, y_valid=y_test)  \n",
    "    feature_impt.reset_index(inplace=True)\n",
    "    \n",
    "    print('feature importance obtained. saving it as a file')\n",
    "\n",
    "    #save fi to dbfs\n",
    "    spark.createDataFrame(feature_impt).coalesce(1).write.format('com.databricks.spark.csv').option('header', 'true').save(f'dbfs:/FileStore/feature_selection_project/{biz_name}_nopromo.csv')\n",
    "    \n",
    "  return feature_impt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "db304916-f886-4e92-a585-422553d2cc28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#4) function to obtain final selected features. here, we select all features with perm imptance > 0. this can be changed in the future as per requried\n",
    "\n",
    "def get_final_features(feature_impt):  \n",
    "  feature_col_list = list(feature_impt['Feature'])\n",
    "    \n",
    "  new_feature_impt = feature_impt[feature_impt['Importance']>0]\n",
    "  selected_features = list(new_feature_impt['Feature'])\n",
    "    \n",
    "  #1) working with dfp\n",
    "  if 'all_sales_cols' in feature_col_list:\n",
    "    if 'all_sales_cols' in selected_features:\n",
    "      print(f'number of features selected: {len(new_feature_impt)+13} out of {len(feature_impt)+13}')  #add 13 because there are 13 sales columns\n",
    "      final_selected_features = []\n",
    "      for feature in selected_features:\n",
    "        if feature == 'all_sales_cols':\n",
    "          final_selected_features.extend([col for col in dfp.columns if 'sales' in col]) \n",
    "        else:\n",
    "          final_selected_features.append(feature)\n",
    "\n",
    "    else:\n",
    "      print(f'number of features selected: {len(new_feature_impt)} out of {len(feature_impt)+13}')\n",
    "      final_selected_features = selected_features\n",
    "      \n",
    "  #2) working with dfnp\n",
    "  else:\n",
    "    print(f'number of features selected: {len(new_feature_impt)} out of {len(feature_impt)}')\n",
    "    final_selected_features = selected_features\n",
    "  \n",
    "  return final_selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "86098b5b-50b0-4b65-954c-8ed48b2a23d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#example of the flow of using the above functions, on USR business\n",
    "\n",
    "biz_name = 'usr'\n",
    "(dfp, dfnp) = obtain_cleaned_dfp_dfnp(usr)  #note: usr is a spark dataframe of the file sales_history_sell_out_usr.csv\n",
    "\n",
    "for df in [dfp, dfnp]:\n",
    "  ((opt_max_depth, opt_min_samples_leaf, opt_min_samples_split, opt_criterion), (X_train, X_test, y_train, y_test))= run_rf(df)  \n",
    "  feature_impt = get_perm_imptance(df, opt_max_depth, opt_min_samples_leaf, opt_min_samples_split, opt_criterion, X_train, X_test, y_train, y_test, biz_name)\n",
    "  final_selected_features = get_final_features(fi)\n",
    "  print(final_selected_features)  #print to see the final selected features\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Phase 1 Codes (Feature Selection Technique)",
   "notebookOrigID": 593916732655262,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
